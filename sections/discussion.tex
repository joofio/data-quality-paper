The first thing to address is that data quality is still an elusive concept since it has a contextual dimension and the quality of the record depends on the usage of the information. For example, data aimed at primary usage and day-to-day healthcare decisions about a patient will have different requirements regarding the importance of some variable or completeness of information very different from data needed to create summary statistics for key performance indicators extraction. 
Moreover, the data is still very vendor-specific. Even though we used an interoperability standard, the semantic layer, more connected with terminology is still lacking. This is an issue to be addressed in order to improve the interoperability of the standard. Moreover, we do not know how the training done with this data is generalizable to other vendors. One opportunity arises of mapping all of this data to a widely used terminology like snomed or loinc. Nevertheless, the usage of FHIR and the fact that the data is mapped to a standard terminology, makes it easier to use the data in other systems and to compare the results with other studies. Furthermore, being available freely and online makes it easier to understand how to map vendor specific datasets to the model and use it in other contexts.


%discutir que dependendendo da pessoa /context/profissao/objecti pode ser preciso dar pesos diferentes a diferentes colunas.
%para dar GDH nao ha coisas que sao precisas mas ha coisas vitais
%para financeiro ha so umas precisas (tipo meds e procs)
%para clinico ha outras e por ai fora

Regarding the clinical evaluation, our results showed great variability between clinicians and between clinicians and the model. This was already expected since data quality is a quite evasive concept and highly contextual and subjective evaluation. However, we do see a tendency for a better assessment of the worst quality results than the best ones. This is also related to comments we received from evaluators, where they could benefit from marking records of equal quality in their perspective, and being unable to do so, they would rely on guts or intuition to make the decision. This is a very important aspect to take into account when designing the evaluation of data quality.
This result suggests that the system may not be suitable to classify and rank good-quality records but could be useful to alert for low-quality ones, which is also a very important task with a great impact on the quality of the data. These findings are supported not only by \ref*{fig:clinical} but also by figure \ref*{fig:scores} where we can add some threshold for the need for a human review. From the preliminary data in the questionnaires and looking at the graph, we believe that a threshold of around 0.3 could be a good starting point. This is a very important aspect to take into account when designing the evaluation of data quality.
Aligned with this is the fact that the system relies on existing methods of trying to explain the data, and outlier-tree and the bayesian network are vital to that task. If explainability and interpretability are important, this need only increases when we are dealing with such subjective concepts as data quality.
However, it is not perfect and it is not always able to explain the data. This is a limitation of the system and it is important to address it in future work.