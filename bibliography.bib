@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@inproceedings{pgmpy,
  title={pgmpy: Probabilistic graphical models using python},
  author={Ankan, Ankur and Panda, Abinash},
  booktitle={Proceedings of the 14th Python in Science Conference (SCIPY 2015)},
  year={2015},
  organization={Citeseer}
}

@article{martin-sanchezBigDataMedicine2014,
  title = {Big Data in Medicine Is Driving Big Changes},
  author = {{Martin-Sanchez}, F. and Verspoor, K.},
  year = {2014},
  month = aug,
  journal = {Yearbook of Medical Informatics},
  volume = {9},
  pages = {14--20},
  issn = {2364-0502},
  doi = {10.15265/IY-2014-0020},
  abstract = {OBJECTIVES: To summarise current research that takes advantage of "Big Data" in health and biomedical informatics applications. METHODS: Survey of trends in this work, and exploration of literature describing how large-scale structured and unstructured data sources are being used to support applications from clinical decision making and health policy, to drug design and pharmacovigilance, and further to systems biology and genetics. RESULTS: The survey highlights ongoing development of powerful new methods for turning that large-scale, and often complex, data into information that provides new insights into human health, in a range of different areas. Consideration of this body of work identifies several important paradigm shifts that are facilitated by Big Data resources and methods: in clinical and translational research, from hypothesis-driven research to data-driven research, and in medicine, from evidence-based practice to practice-based evidence. CONCLUSIONS: The increasing scale and availability of large quantities of health data require strategies for data management, data linkage, and data integration beyond the limits of many existing information systems, and substantial effort is underway to meet those needs. As our ability to make sense of that data improves, the value of the data will continue to increase. Health systems, genetics and genomics, population and public health; all areas of biomedicine stand to benefit from Big Data and the associated technologies.},
  langid = {english},
  pmcid = {PMC4287083},
  pmid = {25123716},
  keywords = {aumento dados his},
  file = {/Users/joaoalmeida/Zotero/storage/ZQ2FWPUR/Martin-Sanchez and Verspoor - 2014 - Big data in medicine is driving big changes.pdf}
}

@article{kramerImpactDataQuality2021,
  title = {The Impact of Data Quality Defects on Clinical Decision-Making in the Intensive Care Unit},
  author = {Kramer, Oren and Even, Adir and Matot, Idit and Steinberg, Yohai and Bitan, Yuval},
  year = {2021},
  month = sep,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {209},
  pages = {106359},
  issn = {1872-7565},
  doi = {10.1016/j.cmpb.2021.106359},
  abstract = {OBJECTIVE: Poor clinical data quality might affect clinical decision making and patient treatment. This study identifies quality defects in clinical data collected automatically by bedside monitoring devices in the Intensive Care Unit (ICU) and examines their effect on clinical decisions. METHODS: Real-world data collected from 7688 patients admitted to the general ICU in a tertiary referral hospital over seven years was retrospectively analyzed. Data quality defect detection methods that use time-series analysis techniques identified two types of data quality defects: (a) completeness: the extent of non-missing values, and (b) validity: the extent of non-extreme values within the continuous range of values. Data quality defects were compared to five scenarios of medication and procedure prescriptions that are common in ICU settings: Blood-pressure reduction, blood-pressure elevation, anesthesia medications, intubation procedures, and muscle relaxant medications. RESULTS: Results from a logistic regression revealed a strong connection between data quality and the clinical interventions examined: lower validity level increased the likelihood of prescription decisions for all five scenarios, and lower completeness level increased the likelihood of prescription decisions for some scenarios. DISCUSSION: The results highlight the possible effect of data quality defects on physicians' decisions. Lower validity of certain key clinical parameters, and in some scenarios lower completeness, correlated with stronger tendency to prescribe medications or perform invasive procedures. CONCLUSIONS: Data quality defects in clinical data affect decision making even without practitioners' awareness. Thus, it is important to emphasize these effects to ICU staff, as well as to medical device manufacturers.},
  langid = {english},
  pmid = {34438224},
  keywords = {_tablet,notion},
  file = {/Users/joaoalmeida/Zotero/storage/692RZ3C5/Kramer et al_2021_The impact of data quality defects on clinical decision-making in the intensive.pdf}
}



@article{gigantiImpactDataQuality2019,
  title = {The Impact of Data Quality and Source Data Verification on Epidemiologic Inference: A Practical Application Using {{HIV}} Observational Data},
  shorttitle = {The Impact of Data Quality and Source Data Verification on Epidemiologic Inference},
  author = {Giganti, Mark J. and Shepherd, Bryan E. and {Caro-Vega}, Yanink and Luz, Paula M. and Rebeiro, Peter F. and Maia, Marcelle and Julmiste, Gaetane and Cortes, Claudia and McGowan, Catherine C. and Duda, Stephany N.},
  year = {2019},
  month = dec,
  journal = {BMC public health},
  volume = {19},
  number = {1},
  pages = {1748},
  issn = {1471-2458},
  doi = {10.1186/s12889-019-8105-2},
  abstract = {BACKGROUND: Data audits are often evaluated soon after completion, even though the identification of systematic issues may lead to additional data quality improvements in the future. In this study, we assess the impact of the entire data audit process on subsequent statistical analyses. METHODS: We conducted on-site audits of datasets from nine international HIV care sites. Error rates were quantified for key demographic and clinical variables among a subset of records randomly selected for auditing. Based on audit results, some sites were tasked with targeted validation of high-error-rate variables resulting in a post-audit dataset. We estimated the times from antiretroviral therapy initiation until death and first AIDS-defining event using the pre-audit data, the audit data, and the post-audit data. RESULTS: The overall discrepancy rate between pre-audit and audit data (n\,=\,250) across all audited variables was 17.1\%. The estimated probability of mortality and an AIDS-defining event over time was higher in the audited data relative to the pre-audit data. Among patients represented in both the post-audit and pre-audit cohorts (n\,=\,18,999), AIDS and mortality estimates also were higher in the post-audit data. CONCLUSION: Though some changes may have occurred independently, our findings suggest that improved data quality following the audit may impact epidemiological inferences.},
  langid = {english},
  pmcid = {PMC6937856},
  pmid = {31888571},
  keywords = {_tablet,notion},
  file = {/Users/joaoalmeida/Zotero/storage/24ZMTKK9/Giganti et al_2019_The impact of data quality and source data verification on epidemiologic.pdf}
}





@article{wengClinicalDataQuality2020,
  title = {Clinical Data Quality: A Data Life Cycle Perspective},
  shorttitle = {Clinical Data Quality},
  author = {Weng, Chunhua},
  year = {2020},
  month = jan,
  journal = {Biostatistics \& Epidemiology},
  volume = {4},
  number = {1},
  pages = {6--14},
  publisher = {{Taylor \& Francis}},
  issn = {2470-9360},
  doi = {10.1080/24709360.2019.1572344},
  url = {https://doi.org/10.1080/24709360.2019.1572344},
  urldate = {2022-08-18},
  abstract = {Clinical data is the staple of modern learning health systems. It promises to accelerate biomedical discovery and improves the efficiency of clinical and translational research but is also fraught with significant data quality issues. This paper aims to provide a life cycle perspective of clinical data quality issues along with recommendations for establishing appropriate expectations for research based on real-world clinical data and best practices for reusing clinical data as a secondary data source.},
  keywords = {_tablet,Clinical data,data quality,learning health system,notion},
  annotation = {\_eprint: https://doi.org/10.1080/24709360.2019.1572344},
  file = {/Users/joaoalmeida/Zotero/storage/LGHKQKGI/Weng_2020_Clinical data quality.pdf}
}



@article{reimerDataQualityAssessment2016a,
  title = {Data Quality Assessment Framework to Assess Electronic Medical Record Data for Use in Research},
  author = {Reimer, Andrew P. and Milinovich, Alex and Madigan, Elizabeth A.},
  year = {2016},
  month = jun,
  journal = {International Journal of Medical Informatics},
  volume = {90},
  pages = {40--47},
  issn = {1872-8243},
  doi = {10.1016/j.ijmedinf.2016.03.006},
  abstract = {INTRODUCTION: The proliferation and use of electronic medical records (EMR) in the clinical setting now provide a rich source of clinical data that can be leveraged to support research on patient outcomes, comparative effectiveness, and health systems research. Once the large volume and variety of data that robust clinical EMRs provide is aggregated, the suitability of the data for research purposes must be addressed. Therefore, the purpose of this paper is two-fold. First, we present a stepwise framework capable of guiding initial data quality assessment when matching multiple data sources regardless of context or application. Then, we demonstrate a use case of initial analysis of a longitudinal data repository of electronic health record data that illustrates the first four steps of the framework, and report results. METHODS: A six-step data quality assessment framework is proposed and described that includes the following data quality assessment steps: (1) preliminary analysis, (2) documentation-longitudinal concordance, (3) breadth, (4) data element presence, (5) density, and (6) prediction. The six-step framework was applied to the Transport Data Mart-a data repository that contains over 28,000 records for patients that underwent interhospital transfer that includes EMRs from the sending hospitalization, transport, and receiving hospitalization. RESULTS: There were a total of 9557 log entries of which 8139 were successfully matched to corresponding hospital encounters. 2832 were successfully mapped to both the sending and receiving hospital encounters (resulting in a 93\% automatic matching rate), with 590 including air medical transport EMR data representing a complete case for testing. Results from Step 2 indicate that once records are identified and matched, there appears to be relatively limited drop-off of additional records when the criteria for matching increases, indicating the a proportion of records consistently contain nearly complete data. Measures of central tendency used in Step 3 and 4 exhibit a right skewness suggesting that a small proportion of records contain the highest number of repeated measures for the measured variables. CONCLUSIONS: The proposed six-step data quality assessment framework is useful in establishing the metadata for a longitudinal data repository that can be replicated by other studies. There are practical issues that need to be addressed including the data quality assessments-with the most prescient being the need to establish data quality metrics for benchmarking acceptable levels of EMR data inclusiveness through testing and application.},
  langid = {english},
  pmcid = {PMC4845906},
  pmid = {27103196},
  keywords = {Biomedical Research,Data Accuracy,Documentation,Electronic Health Records,Electronic medical records,Evaluation \& assessment,Humans,Information storage,Information Storage and Retrieval,Metadata,notion,Retrieval \& integration},
  file = {/Users/joaoalmeida/Zotero/storage/QSK5XDVR/2016_Data quality assessment framework to assess electronic medical record data forReimer et al_.pdf}
}





@article{joukesImpactElectronicPaperBased2019a,
  title = {Impact of {{Electronic}} versus {{Paper-Based Recording}} before {{EHR Implementation}} on {{Health Care Professionals}}' {{Perceptions}} of {{EHR Use}}, {{Data Quality}}, and {{Data Reuse}}},
  author = {Joukes, Erik and {de Keizer}, Nicolette F. and {de Bruijne}, Martine C. and {Abu-Hanna}, Ameen and Cornet, Ronald},
  year = {2019},
  month = mar,
  journal = {Applied Clinical Informatics},
  volume = {10},
  number = {2},
  pages = {199--209},
  issn = {1869-0327},
  doi = {10.1055/s-0039-1681054},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6426723/},
  urldate = {2022-08-25},
  abstract = {Background {$\quad$}The implementation of an electronic health record (EHR) with structured and standardized recording of patient data can improve data quality and reusability. Whether and how users perceive these advantages may depend on the preimplementation situation. ,  Objective {$\quad$}To determine whether the influence of implementing a structured and standardized EHR on perceived EHR use, data quality, and data reuse differed for users working with paper-based records versus a legacy EHR before implementation. ,  Methods {$\quad$}We used an electronic questionnaire to measure users' perception before implementation (2014), expected change, and perceived change after implementation (2016) on three themes. We included all health care professionals in two university hospitals in the Netherlands. Before jointly implementing the same structured and standardized EHR, one hospital used paper-based records and the other a legacy EHR. We compared perceptions before and after implementation for both centers. Additionally, we compared expected benefit with perceived benefit. ,  Results {$\quad$}We received 7,611 responses (4,537 before and 3,074 after implementation) of which 5,707 (75\%) were from professionals reading and recording patient data. A total of 975 (13\%) professionals responded to both before and after implementation questionnaires. In the formerly paper-based center staff perceived improvement in all themes after implementation. The legacy EHR center experienced deterioration of perceived EHR use and data reuse, and only one improvement in EHR use. In both centers, for half of the aspects at least 45\% of responders experienced results worse than expected preimplementation. ,  Conclusion {$\quad$}Our results indicate that the preimplementation recording practice impacts the perceived effect of the implementation of a structured and standardized EHR. For almost half of the respondents the new EHR did not meet their expectations. Especially legacy EHR centers need to investigate the expectations as these might be different and less clear cut than those in paper-based centers. These expectations need to be addressed appropriately to achieve a successful implementation.},
  pmcid = {PMC6426723},
  pmid = {30895574},
  file = {/Users/joaoalmeida/Zotero/storage/59D2VJ33/2019_Impact of Electronic versus Paper-Based Recording before EHR Implementation onJoukes et al_.pdf}
}



@article{huserMultisiteEvaluationData2016,
  title = {Multisite {{Evaluation}} of a {{Data Quality Tool}} for {{Patient-Level Clinical Data Sets}}},
  author = {Huser, Vojtech and DeFalco, Frank J. and Schuemie, Martijn and Ryan, Patrick B. and Shang, Ning and Velez, Mark and Park, Rae Woong and Boyce, Richard D. and Duke, Jon and Khare, Ritu and Utidjian, Levon and Bailey, Charles},
  year = {2016},
  journal = {EGEMS (Washington, DC)},
  volume = {4},
  number = {1},
  pages = {1239},
  issn = {2327-9214},
  doi = {10.13063/2327-9214.1239},
  abstract = {INTRODUCTION: Data quality and fitness for analysis are crucial if outputs of analyses of electronic health record data or administrative claims data should be trusted by the public and the research community. METHODS: We describe a data quality analysis tool (called Achilles Heel) developed by the Observational Health Data Sciences and Informatics Collaborative (OHDSI) and compare outputs from this tool as it was applied to 24 large healthcare datasets across seven different organizations. RESULTS: We highlight 12 data quality rules that identified issues in at least 10 of the 24 datasets and provide a full set of 71 rules identified in at least one dataset. Achilles Heel is a freely available software that provides a useful starter set of data quality rules with the ability to add additional rules. We also present results of a structured email-based interview of all participating sites that collected qualitative comments about the value of Achilles Heel for data quality evaluation. DISCUSSION: Our analysis represents the first comparison of outputs from a data quality tool that implements a fixed (but extensible) set of data quality rules. Thanks to a common data model, we were able to compare quickly multiple datasets originating from several countries in America, Europe and Asia.},
  langid = {english},
  pmcid = {PMC5226382},
  pmid = {28154833},
  file = {/Users/joaoalmeida/Zotero/storage/FN5TCCHH/Huser et al. - 2016 - Multisite Evaluation of a Data Quality Tool for Pa.pdf}
}



@article{zhangUnderstandingDetectingDefects2020,
  title = {Understanding and Detecting Defects in Healthcare Administration Data: {{Toward}} Higher Data Quality to Better Support Healthcare Operations and Decisions},
  shorttitle = {Understanding and Detecting Defects in Healthcare Administration Data},
  author = {Zhang, Yili and Koru, G{\"u}ne{\c s}},
  year = {2020},
  month = mar,
  journal = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {27},
  number = {3},
  pages = {386--395},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocz201},
  abstract = {OBJECTIVE: Development of systematic approaches for understanding and assessing data quality is becoming increasingly important as the volume and utilization of health data steadily increases. In this study, a taxonomy of data defects was developed and utilized when automatically detecting defects to assess Medicaid data quality maintained by one of the states in the United States. MATERIALS AND METHODS: There were more than 2.23 million rows and 32 million cells in the Medicaid data examined. The taxonomy was developed through document review, descriptive data analysis, and literature review. A software program was created to automatically detect defects by using a set of constraints whose development was facilitated by the taxonomy. RESULTS: Five major categories and seventeen subcategories of defects were identified. The major categories are missingness, incorrectness, syntax violation, semantic violation, and duplicity. More than 3 million defects were detected indicating substantial problems with data quality. Defect density exceeded 10\% in five tables. The majority of the data defects belonged to format mismatch, invalid code, dependency-contract violation, and implausible value types. Such contextual knowledge can support prioritized quality improvement initiatives for the Medicaid data studied. CONCLUSIONS: This research took the initial steps to understand the types of data defects and detect defects in large healthcare datasets. The results generally suggest that healthcare organizations can potentially benefit from focusing on data quality improvement. For those purposes, the taxonomy developed and the approach followed in this study can be adopted.},
  langid = {english},
  pmcid = {PMC7647275},
  pmid = {31841149},
  keywords = {Data Accuracy,data defect,data quality,Datasets as Topic,defect taxonomy,Delivery of Health Care,healthcare administration,Humans,Medicaid,Medicaid management information system,notion,Quality Improvement,Software,United States},
  file = {/Users/joaoalmeida/Zotero/storage/FHEERD9C/2020_Understanding and detecting defects in healthcare administration dataZhang_Koru_.pdf}
}



@article{kahnHarmonizedDataQuality2016a,
  title = {A {{Harmonized Data Quality Assessment Terminology}} and {{Framework}} for the {{Secondary Use}} of {{Electronic Health Record Data}}},
  author = {Kahn, Michael G. and Callahan, Tiffany J. and Barnard, Juliana and Bauck, Alan E. and Brown, Jeff and Davidson, Bruce N. and Estiri, Hossein and Goerg, Carsten and Holve, Erin and Johnson, Steven G. and Liaw, Siaw-Teng and {Hamilton-Lopez}, Marianne and Meeker, Daniella and Ong, Toan C. and Ryan, Patrick and Shang, Ning and Weiskopf, Nicole G. and Weng, Chunhua and Zozus, Meredith N. and Schilling, Lisa},
  year = {2016},
  month = sep,
  journal = {eGEMs},
  volume = {4},
  number = {1},
  pages = {1244},
  issn = {2327-9214},
  doi = {10.13063/2327-9214.1244},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5051581/},
  urldate = {2022-08-25},
  abstract = {Objective: Harmonized data quality (DQ) assessment terms, methods, and reporting practices can establish a common understanding of the strengths and limitations of electronic health record (EHR) data for operational analytics, quality improvement, and research. Existing published DQ terms were harmonized to a comprehensive unified terminology with definitions and examples and organized into a conceptual framework to support a common approach to defining whether EHR data is `fit' for specific uses. Materials and Methods: DQ publications, informatics and analytics experts, managers of established DQ programs, and operational manuals from several mature EHR-based research networks were reviewed to identify potential DQ terms and categories. Two face-to-face stakeholder meetings were used to vet an initial set of DQ terms and definitions that were grouped into an overall conceptual framework. Feedback received from data producers and users was used to construct a draft set of harmonized DQ terms and categories. Multiple rounds of iterative refinement resulted in a set of terms and organizing framework consisting of DQ categories, subcategories, terms, definitions, and examples. The harmonized terminology and logical framework's inclusiveness was evaluated against ten published DQ terminologies. Results: Existing DQ terms were harmonized and organized into a framework by defining three DQ categories: (1) Conformance (2) Completeness and (3) Plausibility and two DQ assessment contexts: (1) Verification and (2) Validation. Conformance and Plausibility categories were further divided into subcategories. Each category and subcategory was defined with respect to whether the data may be verified with organizational data, or validated against an accepted gold standard, depending on proposed context and uses. The coverage of the harmonized DQ terminology was validated by successfully aligning to multiple published DQ terminologies. Discussion: Existing DQ concepts, community input, and expert review informed the development of a distinct set of terms, organized into categories and subcategories. The resulting DQ terms successfully encompassed a wide range of disparate DQ terminologies. Operational definitions were developed to provide guidance for implementing DQ assessment procedures. The resulting structure is an inclusive DQ framework for standardizing DQ assessment and reporting. While our analysis focused on the DQ issues often found in EHR data, the new terminology may be applicable to a wide range of electronic health data such as administrative, research, and patient-reported data. Conclusion: A consistent, common DQ terminology, organized into a logical framework, is an initial step in enabling data owners and users, patients, and policy makers to evaluate and communicate data quality findings in a well-defined manner with a shared vocabulary. Future work will leverage the framework and terminology to develop reusable data quality assessment and reporting methods.},
  pmcid = {PMC5051581},
  pmid = {27713905},
  keywords = {_tablet,notion},
  file = {/Users/joaoalmeida/Zotero/storage/6LVU4FKJ/2016_A Harmonized Data Quality Assessment Terminology and Framework for theKahn et al_.pdf}
}



@article{weiskopfMethodsDimensionsElectronic2013,
  title = {Methods and Dimensions of Electronic Health Record Data Quality Assessment: Enabling Reuse for Clinical Research},
  shorttitle = {Methods and Dimensions of Electronic Health Record Data Quality Assessment},
  author = {Weiskopf, Nicole Gray and Weng, Chunhua},
  year = {2013},
  month = jan,
  journal = {Journal of the American Medical Informatics Association},
  volume = {20},
  number = {1},
  pages = {144--151},
  issn = {1067-5027},
  doi = {10.1136/amiajnl-2011-000681},
  url = {https://doi.org/10.1136/amiajnl-2011-000681},
  urldate = {2022-02-22},
  abstract = {Objective To review the methods and dimensions of data quality assessment in the context of electronic health record (EHR) data reuse for research.Materials and methods A review of the clinical research literature discussing data quality assessment methodology for EHR data was performed. Using an iterative process, the aspects of data quality being measured were abstracted and categorized, as well as the methods of assessment used.Results Five dimensions of data quality were identified, which are completeness, correctness, concordance, plausibility, and currency, and seven broad categories of data quality assessment methods: comparison with gold standards, data element agreement, data source agreement, distribution comparison, validity checks, log review, and element presence.Discussion Examination of the methods by which clinical researchers have investigated the quality and suitability of EHR data for research shows that there are fundamental features of data quality, which may be difficult to measure, as well as proxy dimensions. Researchers interested in the reuse of EHR data for clinical research are recommended to consider the adoption of a consistent taxonomy of EHR data quality, to remain aware of the task-dependence of data quality, to integrate work on data quality assessment from other fields, and to adopt systematic, empirically driven, statistically based methods of data quality assessment.Conclusion There is currently little consistency or potential generalizability in the methods used to assess EHR data quality. If the reuse of EHR data for clinical research is to become accepted, researchers should adopt validated, systematic methods of EHR data quality assessment.},
  keywords = {_tablet_modified,Data Collection,Electronic Health Records,Humans,Information Dissemination,notion,Quality Control,Reproducibility of Results,Research Design,VIP},
  file = {/Users/joaoalmeida/Zotero/storage/6H5FBQZG/Weiskopf and Weng - 2013 - Methods and dimensions of electronic health record.pdf;/Users/joaoalmeida/Zotero/storage/7XJFPYRB/2909176.html}
}


@article{bianAssessingPracticeData2020,
  title = {Assessing the Practice of Data Quality Evaluation in a National Clinical Data Research Network through a Systematic Scoping Review in the Era of Real-World Data},
  author = {Bian, Jiang and Lyu, Tianchen and Loiacono, Alexander and Viramontes, Tonatiuh Mendoza and Lipori, Gloria and Guo, Yi and Wu, Yonghui and Prosperi, Mattia and George, Thomas J. and Harle, Christopher A. and Shenkman, Elizabeth A. and Hogan, William},
  year = {2020},
  month = dec,
  journal = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {27},
  number = {12},
  pages = {1999--2010},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocaa245},
  abstract = {OBJECTIVE: To synthesize data quality (DQ) dimensions and assessment methods of real-world data, especially electronic health records, through a systematic scoping review and to assess the practice of DQ assessment in the national Patient-centered Clinical Research Network (PCORnet). MATERIALS AND METHODS: We started with 3 widely cited DQ literature-2 reviews from Chan et al (2010) and Weiskopf et al (2013a) and 1 DQ framework from Kahn et al (2016)-and expanded our review systematically to cover relevant articles published up to February 2020. We extracted DQ dimensions and assessment methods from these studies, mapped their relationships, and organized a synthesized summarization of existing DQ dimensions and assessment methods. We reviewed the data checks employed by the PCORnet and mapped them to the synthesized DQ dimensions and methods. RESULTS: We analyzed a total of 3 reviews, 20 DQ frameworks, and 226 DQ studies and extracted 14 DQ dimensions and 10 assessment methods. We found that completeness, concordance, and correctness/accuracy were commonly assessed. Element presence, validity check, and conformance were commonly used DQ assessment methods and were the main focuses of the PCORnet data checks. DISCUSSION: Definitions of DQ dimensions and methods were not consistent in the literature, and the DQ assessment practice was not evenly distributed (eg, usability and ease-of-use were rarely discussed). Challenges in DQ assessments, given the complex and heterogeneous nature of real-world data, exist. CONCLUSION: The practice of DQ assessment is still limited in scope. Future work is warranted to generate understandable, executable, and reusable DQ measures.},
  langid = {english},
  pmcid = {PMC7727392},
  pmid = {33166397},
  keywords = {_tablet,notion},
  file = {/Users/joaoalmeida/Zotero/storage/HJD6X7C8/Bian et al_2020_Assessing the practice of data quality evaluation in a national clinical data.pdf;/Users/joaoalmeida/Zotero/storage/MN8AIL2K/Bian et al_2020_Assessing the practice of data quality evaluation in a national clinical data.pdf}
}




@article{waljiElectronicHealthRecords2019,
  title = {Electronic {{Health Records}} and {{Data Quality}}},
  author = {Walji, Muhammad F.},
  year = {2019},
  month = mar,
  journal = {Journal of Dental Education},
  volume = {83},
  number = {3},
  pages = {263--264},
  issn = {1930-7837},
  doi = {10.21815/JDE.019.034},
  langid = {english},
  pmid = {30824567},
  keywords = {_tablet,notion}
}



@article{verheijPossibleSourcesBias2018,
  title = {Possible {{Sources}} of {{Bias}} in {{Primary Care Electronic Health Record Data Use}} and {{Reuse}}},
  author = {Verheij, Robert A. and Curcin, Vasa and Delaney, Brendan C. and McGilchrist, Mark M.},
  year = {2018},
  month = may,
  journal = {Journal of Medical Internet Research},
  volume = {20},
  number = {5},
  pages = {e9134},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/jmir.9134},
  url = {https://www.jmir.org/2018/5/e185},
  urldate = {2022-08-18},
  langid = {english},
  keywords = {_tablet,notion}
}


@article{coreyAssessingQualitySurgical2020,
  title = {Assessing {{Quality}} of {{Surgical Real-World Data}} from an {{Automated Electronic Health Record Pipeline}}},
  author = {Corey, Kristin M. and Helmkamp, Joshua and Simons, Morgan and Curtis, Lesley and Marsolo, Keith and Balu, Suresh and Gao, Michael and Nichols, Marshall and Watson, Joshua and Mureebe, Leila and Kirk, Allan D. and Sendak, Mark},
  year = {2020},
  month = mar,
  journal = {Journal of the American College of Surgeons},
  volume = {230},
  number = {3},
  pages = {295-305.e12},
  issn = {1879-1190},
  doi = {10.1016/j.jamcollsurg.2019.12.005},
  langid = {english},
  pmid = {31945461},
  keywords = {_tablet,notion}
}


@article{phanAutomatedDataCleaning2020,
  title = {Automated Data Cleaning of Paediatric Anthropometric Data from Longitudinal Electronic Health Records: Protocol and Application to a Large Patient Cohort},
  shorttitle = {Automated Data Cleaning of Paediatric Anthropometric Data from Longitudinal Electronic Health Records},
  author = {Phan, Hang T. T. and Borca, Florina and Cable, David and Batchelor, James and Davies, Justin H. and Ennis, Sarah},
  year = {2020},
  month = jun,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {10164},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-66925-7},
  url = {https://www.nature.com/articles/s41598-020-66925-7},
  urldate = {2022-04-18},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {_tablet,notion}
}


@article{schmidtFacilitatingHarmonizedData2021,
  title = {Facilitating Harmonized Data Quality Assessments. {{A}} Data Quality Framework for Observational Health Research Data Collections with Software Implementations in {{R}}},
  author = {Schmidt, Carsten Oliver and Struckmann, Stephan and Enzenbach, Cornelia and Reineke, Achim and Stausberg, J{\"u}rgen and Damerow, Stefan and Huebner, Marianne and Schmidt, B{\"o}rge and Sauerbrei, Willi and Richter, Adrian},
  year = {2021},
  month = apr,
  journal = {BMC medical research methodology},
  volume = {21},
  number = {1},
  pages = {63},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01252-7},
  langid = {english},
  pmcid = {PMC8019177},
  pmid = {33810787},
  keywords = {_tablet,notion}
}


@article{razzaghiDevelopingSystematicApproach2022,
  title = {Developing a Systematic Approach to Assessing Data Quality in Secondary Use of Clinical Data Based on Intended Use},
  author = {Razzaghi, Hanieh and Greenberg, Jane and Bailey, L. Charles},
  year = {2022},
  journal = {Learning Health Systems},
  volume = {6},
  number = {1},
  pages = {e10264},
  issn = {2379-6146},
  doi = {10.1002/lrh2.10264},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/lrh2.10264},
  urldate = {2022-02-21},
  langid = {english},
  keywords = {_tablet,notion},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/lrh2.10264}
}

@article{rajanContentAgnosticComputable2019,
  title = {Towards a Content Agnostic Computable Knowledge Repository for Data Quality Assessment},
  author = {Rajan, Naresh Sundar and Gouripeddi, Ramkiran and Mo, Peter and Madsen, Randy K. and Facelli, Julio C.},
  year = {2019},
  month = aug,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {177},
  pages = {193--201},
  issn = {1872-7565},
  doi = {10.1016/j.cmpb.2019.05.017},
  langid = {english},
  pmid = {31319948},
  keywords = {_tablet,notion}
}


@article{kapsnerLinkingConsortiumWideData2021a,
  title = {Linking a {{Consortium-Wide Data Quality Assessment Tool}} with the {{MIRACUM Metadata Repository}}},
  author = {Kapsner, Lorenz A. and Mang, Jonathan M. and Mate, Sebastian and Seuchter, Susanne A. and Vengadeswaran, Abishaa and Bathelt, Franziska and Deppenwiese, Noemi and Kadioglu, Dennis and Kraska, Detlef and Prokosch, Hans-Ulrich},
  year = {2021},
  month = aug,
  journal = {Applied Clinical Informatics},
  volume = {12},
  number = {4},
  pages = {826--835},
  issn = {1869-0327},
  doi = {10.1055/s-0041-1733847},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8387126/},
  urldate = {2022-05-20},
  pmcid = {PMC8387126},
  pmid = {34433217},
  keywords = {_tablet_modified,notion}
}


@article{liawQualityAssessmentRealworld2021,
  title = {Quality Assessment of Real-World Data Repositories across the Data Life Cycle: {{A}} Literature Review},
  shorttitle = {Quality Assessment of Real-World Data Repositories across the Data Life Cycle},
  author = {Liaw, Siaw-Teng and Guo, Jason Guan Nan and Ansari, Sameera and Jonnagaddala, Jitendra and Godinho, Myron Anthony and Borelli, Alder Jose and {de Lusignan}, Simon and Capurro, Daniel and Liyanage, Harshana and Bhattal, Navreet and Bennett, Vicki and Chan, Jaclyn and Kahn, Michael G.},
  year = {2021},
  month = jul,
  journal = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {28},
  number = {7},
  pages = {1591--1599},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocaa340},
  langid = {english},
  pmcid = {PMC8475229},
  pmid = {33496785},
  keywords = {_tablet,notion,VIP}
}

@article{alvarezsanchezTAQIHToolTabular2019,
  title = {{{TAQIH}}, a Tool for Tabular Data Quality Assessment and Improvement in the Context of Health Data},
  author = {{\'A}lvarez S{\'a}nchez, Roberto and Beristain Iraola, Andoni and Epelde Unanue, Gorka and Carlin, Paul},
  year = {2019},
  month = nov,
  journal = {Computer Methods and Programs in Biomedicine},
  series = {{{SI}}: {{Data Quality Assessment}}},
  volume = {181},
  pages = {104824},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2018.12.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260718304188},
  urldate = {2022-02-21},
  langid = {english},
  keywords = {_tablet,notion,VIP}
}


@article{hripcsakObservationalHealthData2015,
  title = {Observational {{Health Data Sciences}} and {{Informatics}} ({{OHDSI}}): {{Opportunities}} for {{Observational Researchers}}},
  shorttitle = {Observational {{Health Data Sciences}} and {{Informatics}} ({{OHDSI}})},
  author = {Hripcsak, George and Duke, Jon D. and Shah, Nigam H. and Reich, Christian G. and Huser, Vojtech and Schuemie, Martijn J. and Suchard, Marc A. and Park, Rae Woong and Wong, Ian Chi Kei and Rijnbeek, Peter R. and {van der Lei}, Johan and Pratt, Nicole and Nor{\'e}n, G. Niklas and Li, Yu-Chuan and Stang, Paul E. and Madigan, David and Ryan, Patrick B.},
  year = {2015},
  journal = {Studies in Health Technology and Informatics},
  volume = {216},
  pages = {574--578},
  issn = {1879-8365},
  langid = {english},
  pmcid = {PMC4815923},
  pmid = {26262116},
  keywords = {Databases; Factual,Health Services Research,Internationality,Medical Informatics,Models; Organizational,Observational Studies as Topic}
}




@software{chenRankbiasedOverlapRBO2023,
  title = {Rank-Biased {{Overlap}} ({{RBO}})},
  author = {Chen, Changyao},
  date = {2023-03-14T02:55:37Z},
  origdate = {2018-02-11T16:32:40Z},
  url = {https://github.com/changyaochen/rbo},
  urldate = {2023-03-16}
}



@article{saezOrganizingDataQuality,
  title = {Organizing {{Data Quality Assessment}} of {{Shifting Biomedical Data}}},
  author = {Sáez, Carlos and Martínez-Miranda, Juan and Robles, Montserrat and García-Gómez, Juan Miguel},
  langid = {english},
  year={2012},
  journal= {Studies in health technology and informatics}
}


@article{estiriSemisupervisedEncodingOutlier2019,
  title = {Semi-Supervised {{Encoding}} for {{Outlier Detection}} in {{Clinical Observation Data}}},
  author = {Estiri, Hossein and Murphy, Shawn N},
  year = {2019},
  date = {2019-11},
  journal = {Computer methods and programs in biomedicine},
  shortjournal = {Comput Methods Programs Biomed},
  volume = {181},
  eprint = {30658851},
  eprinttype = {pmid},
  pages = {104830},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2019.01.002},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10111961/},
  urldate = {2023-10-11},
  pmcid = {PMC10111961},
  keywords = {notion}
}
@article{saezEHRtemporalVariabilityDelineatingTemporal2020,
  title = {{{EHRtemporalVariability}}: Delineating Temporal Data-Set Shifts in Electronic Health Records},
  shorttitle = {{{EHRtemporalVariability}}},
  author = {Sáez, Carlos and Gutiérrez-Sacristán, Alba and Kohane, Isaac and García-Gómez, Juan M and Avillach, Paul},
  date = {2020-07-30},
  year = {2020},
  journal = {GigaScience},
  shortjournal = {Gigascience},
  volume = {9},
  number = {8},
  eprint = {32729900},
  eprinttype = {pmid},
  pages = {giaa079},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giaa079},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7391413/},
  urldate = {2023-10-11},
  pmcid = {PMC7391413},
  keywords = {notion}
}
@misc{wangTheoreticalAnalysisNDCG,
      title={A Theoretical Analysis of NDCG Type Ranking Measures}, 
      author={Yining Wang and Liwei Wang and Yuanzhi Li and Di He and Tie-Yan Liu and Wei Chen},
      year={2013},
      eprint={1304.6480},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{springateREHRPackageManipulating2017,
  title = {{{rEHR}}: {{An R}} Package for Manipulating and Analysing {{Electronic Health Record}} Data},
  shorttitle = {{{rEHR}}},
  author = {Springate, David A. and Parisi, Rosa and Olier, Ivan and Reeves, David and Kontopantelis, Evangelos},
  year = {2017},
  journal = {PloS One},
  shortjournal = {PLoS One},
  volume = {12},
  number = {2},
  eprint = {28231289},
  eprinttype = {pmid},
  pages = {e0171784},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0171784},
  langid = {english},
  pmcid = {PMC5323003},
  keywords = {{Databases, Factual},Electronic Health Records,Humans,Information Storage and Retrieval,notion,Software,Time Factors,Workflow}
}

